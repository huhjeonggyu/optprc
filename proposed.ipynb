{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3810f1d5-6471-41f2-affe-e4552d270352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import random\n",
    "random_seed = 8\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c53989-14ca-4326-a5a1-e9e7d301db1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>거래량</th>\n",
       "      <th>미결제약정</th>\n",
       "      <th>상대가격</th>\n",
       "      <th>잔존만기</th>\n",
       "      <th>rate</th>\n",
       "      <th>kvix</th>\n",
       "      <th>분류</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>이론하한</th>\n",
       "      <th>종가</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.511452e-05</td>\n",
       "      <td>0.007142</td>\n",
       "      <td>0.255172</td>\n",
       "      <td>0.008011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.455170</td>\n",
       "      <td>0</td>\n",
       "      <td>38.332852</td>\n",
       "      <td>0.172045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.456431e-07</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.218227</td>\n",
       "      <td>0.008011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.450472</td>\n",
       "      <td>0</td>\n",
       "      <td>18.342202</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            거래량     미결제약정      상대가격      잔존만기  rate      kvix   분류  \\\n",
       "0  6.511452e-05  0.007142  0.255172  0.008011   1.0  0.130817  1.0   \n",
       "1  8.456431e-07  0.000377  0.218227  0.008011   1.0  0.130817  1.0   \n",
       "\n",
       "   anomaly_score  anomaly       이론하한        종가  \n",
       "0       0.455170        0  38.332852  0.172045  \n",
       "1       0.450472        0  18.342202  0.083333  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('qr.csv',index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531f7b0c-f048-4933-90a5-61c0459c30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['종가','이론하한'],axis=1)\n",
    "y = df[['종가']]\n",
    "\n",
    "X['rate'] *= 0.01\n",
    "X['kvix'] *= 0.01\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X[['거래량','미결제약정','anomaly_score']])\n",
    "X[['거래량','미결제약정','anomaly_score']] = scaler.transform(X[['거래량','미결제약정','anomaly_score']])\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0,shuffle=True)\n",
    "z_train = X_train[['anomaly']]; X_train = X_train.drop('anomaly',axis=1)\n",
    "z_test = X_test[['anomaly']]; X_test = X_test.drop('anomaly',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8e268d-9e11-4035-861e-ee66149529f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>거래량</th>\n",
       "      <th>미결제약정</th>\n",
       "      <th>상대가격</th>\n",
       "      <th>잔존만기</th>\n",
       "      <th>rate</th>\n",
       "      <th>kvix</th>\n",
       "      <th>분류</th>\n",
       "      <th>anomaly_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125750</th>\n",
       "      <td>0.014087</td>\n",
       "      <td>0.041250</td>\n",
       "      <td>0.242525</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.224755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52372</th>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>0.270790</td>\n",
       "      <td>0.076101</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             거래량     미결제약정      상대가격      잔존만기      rate      kvix   분류  \\\n",
       "125750  0.014087  0.041250  0.242525  0.006676  0.000072  0.002284  0.0   \n",
       "52372   0.000086  0.003393  0.270790  0.076101  0.007381  0.000314  0.0   \n",
       "\n",
       "        anomaly_score  \n",
       "125750       0.224755  \n",
       "52372        0.064314  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.sample(frac=1).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c00428-57db-45d4-bb38-3896cdaa9651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>종가</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184071</th>\n",
       "      <td>0.011809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66569</th>\n",
       "      <td>0.027385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              종가\n",
       "184071  0.011809\n",
       "66569   0.027385"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sample(frac=1).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00379d3b-9fac-4843-9fc1-03afb323c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).float()\n",
    "y_test = torch.from_numpy(y_test.values).float()\n",
    "z_train = torch.from_numpy(z_train.values).float()\n",
    "z_test = torch.from_numpy(z_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e96a595-692a-4e81-848e-4915032faa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = torch.utils.data.TensorDataset(X_train,y_train,z_train)\n",
    "dataset_test = torch.utils.data.TensorDataset(X_test,y_test,z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ab60fe-f51c-4440-b4ad-86494be8dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=1000, \n",
    "                                               shuffle=True, num_workers=5, drop_last=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1000, \n",
    "                                              shuffle=False, num_workers=5, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2cd4053-56c1-4b64-aee6-9ead737512f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(model_ano,dataloader,optimizer=None):\n",
    "    \n",
    "    ano_losses = []\n",
    "    ano_losses_0 = []; ano_losses_1 = []\n",
    "    m0 = 0; m1 = 0; m = 0\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        \n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        x_sample, y_sample, z_sample = samples\n",
    "        x_sample = x_sample.cuda()\n",
    "        y_sample = y_sample.cuda()\n",
    "        z_sample = z_sample.cuda()\n",
    "        \n",
    "        n0 = (z_sample==0).sum().item()\n",
    "        n1 = (z_sample==1).sum().item()\n",
    "        idx0 = (z_sample==0).squeeze()\n",
    "        idx1 = (z_sample==1).squeeze()\n",
    "        \n",
    "        m0 += n0\n",
    "        m1 += n1\n",
    "        m  += (n0+n1)\n",
    "            \n",
    "        pred = model_ano(x_sample)\n",
    "        ano_loss_0 = torch.sum((pred[idx0]-y_sample[idx0])**2)\n",
    "        ano_loss_1 = torch.sum((pred[idx1]-y_sample[idx1])**2)\n",
    "        ano_losses_0.append( ano_loss_0.item() )\n",
    "        ano_losses_1.append( ano_loss_1.item() )\n",
    "       \n",
    "        ano_loss = ano_loss_0+ano_loss_1\n",
    "        ano_losses.append( (ano_loss_0+ano_loss_1).item() )\n",
    "        if optimizer :\n",
    "            ano_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return np.sum(ano_losses)/m,np.sum(ano_losses_0)/m0,np.sum(ano_losses_1)/m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0108a8a2-001f-43ca-9da9-28260606336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Ano(nn.Module):\n",
    "    def __init__(self):\n",
    "    \n",
    "        super(Model_Ano, self).__init__()\n",
    "        \n",
    "        self.linear1b = nn.Linear(8,100)\n",
    "        self.linear2b = nn.Linear(100,1)\n",
    "        \n",
    "        self.linear1 = nn.Linear(7,100)\n",
    "        self.linear2 = nn.Linear(100,100)\n",
    "        self.linear3 = nn.Linear(100,1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        e = self.linear1b(x)\n",
    "        e = self.relu(e)\n",
    "        e = self.linear2b(e)\n",
    "        e = self.sigmoid(e)\n",
    "    \n",
    "        c = self.linear1(x[:,:-1])\n",
    "        c = self.relu(c)\n",
    "        c = self.linear2(c)\n",
    "        c = self.relu(c)\n",
    "        c = self.linear3(c)\n",
    "        \n",
    "        return c*e\n",
    "\n",
    "model_ano = Model_Ano()\n",
    "model_ano = model_ano.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model_ano.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,60,100], gamma=0.1, verbose=False)\n",
    "ano_loss_fn = nn.MSELoss()\n",
    "\n",
    "EPOCHS = 150\n",
    "ano_loss_list = np.zeros((EPOCHS,2))\n",
    "ano_loss_list_0 = np.zeros((EPOCHS,2))\n",
    "ano_loss_list_1 = np.zeros((EPOCHS,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a7ea5d2-57c7-4924-8be9-54ed36c45049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  train 2.393091e-03 1.123072e-03 1.085088e-02 0  test 6.576212e-05 5.562454e-05 1.322953e-04\n",
      "1  train 4.163702e-05 3.222863e-05 1.042960e-04 1  test 3.743404e-05 2.604161e-05 1.122028e-04\n",
      "2  train 3.066872e-05 2.291184e-05 8.232877e-05 2  test 2.485060e-05 1.680779e-05 7.763579e-05\n",
      "3  train 3.800401e-05 3.042597e-05 8.848045e-05 3  test 6.710370e-05 5.643374e-05 1.371309e-04\n",
      "4  train 3.359984e-05 2.539776e-05 8.822488e-05 4  test 2.299366e-05 1.520163e-05 7.413291e-05\n",
      "5  train 2.455738e-05 1.803764e-05 6.797608e-05 5  test 2.228446e-05 1.576220e-05 6.509019e-05\n",
      "6  train 3.109663e-05 2.359785e-05 8.104019e-05 6  test 5.479410e-05 3.372333e-05 1.930821e-04\n",
      "7  train 2.856667e-05 2.007062e-05 8.514674e-05 7  test 2.011939e-05 1.316359e-05 6.577049e-05\n",
      "8  train 2.295793e-05 1.664265e-05 6.502531e-05 8  test 2.047518e-05 1.329131e-05 6.762309e-05\n",
      "9  train 2.531348e-05 1.843247e-05 7.114480e-05 9  test 1.824141e-05 1.218854e-05 5.796658e-05\n",
      "10 train 2.692570e-05 1.947801e-05 7.652420e-05 10 test 2.891352e-05 2.366833e-05 6.333787e-05\n",
      "11 train 2.187804e-05 1.567445e-05 6.319128e-05 11 test 1.918323e-05 1.203991e-05 6.606499e-05\n",
      "12 train 2.202080e-05 1.607151e-05 6.165022e-05 12 test 2.034214e-05 1.393960e-05 6.236218e-05\n",
      "13 train 2.041372e-05 1.481086e-05 5.773188e-05 13 test 1.812028e-05 1.216914e-05 5.717770e-05\n",
      "14 train 2.105973e-05 1.540886e-05 5.869584e-05 14 test 2.579806e-05 1.777142e-05 7.847712e-05\n",
      "15 train 1.905101e-05 1.351855e-05 5.590205e-05 15 test 1.770092e-05 1.232688e-05 5.297092e-05\n",
      "16 train 2.461282e-05 1.807437e-05 6.815822e-05 16 test 2.758166e-05 2.243159e-05 6.138169e-05\n",
      "17 train 1.991298e-05 1.424237e-05 5.767869e-05 17 test 1.760358e-05 1.165492e-05 5.664479e-05\n",
      "18 train 2.533629e-05 1.700435e-05 8.083436e-05 18 test 1.849072e-05 1.300956e-05 5.446374e-05\n",
      "19 train 1.869487e-05 1.319406e-05 5.533336e-05 19 test 1.652308e-05 1.050604e-05 5.601308e-05\n",
      "20 train 2.113502e-05 1.517023e-05 6.085798e-05 20 test 4.362399e-05 3.887850e-05 7.476879e-05\n",
      "21 train 2.181315e-05 1.599742e-05 6.053962e-05 21 test 2.731589e-05 1.998712e-05 7.541481e-05\n",
      "22 train 2.317417e-05 1.640376e-05 6.826884e-05 22 test 2.712497e-05 1.796687e-05 8.722987e-05\n",
      "23 train 2.015440e-05 1.457502e-05 5.731802e-05 23 test 1.600711e-05 1.064156e-05 5.122135e-05\n",
      "24 train 3.380822e-05 2.412824e-05 9.827908e-05 24 test 1.969049e-05 1.340333e-05 6.095332e-05\n",
      "25 train 2.314848e-05 1.588147e-05 7.154370e-05 25 test 2.016479e-05 1.353388e-05 6.368363e-05\n",
      "26 train 2.124135e-05 1.441724e-05 6.670036e-05 26 test 1.666122e-05 1.091689e-05 5.436142e-05\n",
      "27 train 1.726591e-05 1.211349e-05 5.158388e-05 27 test 1.624133e-05 1.098927e-05 5.071073e-05\n",
      "28 train 1.860359e-05 1.304528e-05 5.561780e-05 28 test 2.145263e-05 1.491580e-05 6.435401e-05\n",
      "29 train 1.797658e-05 1.285056e-05 5.211369e-05 29 test 2.183077e-05 1.677748e-05 5.499558e-05\n",
      "30 train 1.348938e-05 9.516212e-06 3.995420e-05 30 test 1.470435e-05 9.598521e-06 4.821405e-05\n",
      "31 train 1.315084e-05 9.255342e-06 3.909959e-05 31 test 1.479159e-05 9.720923e-06 4.807050e-05\n",
      "32 train 1.310773e-05 9.241426e-06 3.885823e-05 32 test 1.573331e-05 1.054589e-05 4.977847e-05\n",
      "33 train 1.325728e-05 9.340668e-06 3.934536e-05 33 test 1.509046e-05 9.920255e-06 4.902267e-05\n",
      "34 train 1.305618e-05 9.198823e-06 3.874703e-05 34 test 1.474604e-05 9.526763e-06 4.900027e-05\n",
      "35 train 1.304925e-05 9.194167e-06 3.872495e-05 35 test 1.504464e-05 9.594224e-06 5.081582e-05\n",
      "36 train 1.307779e-05 9.220857e-06 3.876459e-05 36 test 1.491943e-05 9.568915e-06 5.003500e-05\n",
      "37 train 1.297489e-05 9.170439e-06 3.831961e-05 37 test 1.447104e-05 9.370388e-06 4.794671e-05\n",
      "38 train 1.320843e-05 9.315255e-06 3.913406e-05 38 test 1.483601e-05 9.514183e-06 4.976333e-05\n",
      "39 train 1.307837e-05 9.234205e-06 3.867633e-05 39 test 1.457862e-05 9.460873e-06 4.816649e-05\n",
      "40 train 1.318565e-05 9.267848e-06 3.927652e-05 40 test 1.481571e-05 9.719867e-06 4.825983e-05\n",
      "41 train 1.302409e-05 9.190345e-06 3.856146e-05 41 test 1.440178e-05 9.295106e-06 4.791702e-05\n",
      "42 train 1.334437e-05 9.424581e-06 3.945104e-05 42 test 1.735414e-05 1.078703e-05 6.045424e-05\n",
      "43 train 1.327380e-05 9.394038e-06 3.911260e-05 43 test 1.467788e-05 9.367250e-06 4.953169e-05\n",
      "44 train 1.302836e-05 9.209981e-06 3.845838e-05 44 test 1.512288e-05 9.825187e-06 4.989178e-05\n",
      "45 train 1.314568e-05 9.292598e-06 3.881185e-05 45 test 1.476473e-05 9.607171e-06 4.861392e-05\n",
      "46 train 1.289128e-05 9.106837e-06 3.809409e-05 46 test 1.485425e-05 9.481731e-06 5.011421e-05\n",
      "47 train 1.296779e-05 9.219207e-06 3.793299e-05 47 test 1.458406e-05 9.542431e-06 4.767242e-05\n",
      "48 train 1.310890e-05 9.292300e-06 3.852835e-05 48 test 1.514475e-05 1.003465e-05 4.868252e-05\n",
      "49 train 1.302022e-05 9.268014e-06 3.801079e-05 49 test 1.439740e-05 9.240729e-06 4.824073e-05\n",
      "50 train 1.299685e-05 9.153678e-06 3.859076e-05 50 test 1.574758e-05 9.576170e-06 5.625070e-05\n",
      "51 train 1.310481e-05 9.299945e-06 3.844236e-05 51 test 1.420360e-05 9.108110e-06 4.764543e-05\n",
      "52 train 1.289779e-05 9.129006e-06 3.799510e-05 52 test 1.492223e-05 9.464363e-06 5.074234e-05\n",
      "53 train 1.302950e-05 9.211985e-06 3.845629e-05 53 test 1.451662e-05 9.265435e-06 4.898030e-05\n",
      "54 train 1.292728e-05 9.146635e-06 3.810599e-05 54 test 1.488779e-05 9.417873e-06 5.078701e-05\n",
      "55 train 1.299781e-05 9.236511e-06 3.805017e-05 55 test 1.563484e-05 9.777140e-06 5.407905e-05\n",
      "56 train 1.315312e-05 9.352241e-06 3.846536e-05 56 test 1.523814e-05 9.850768e-06 5.059563e-05\n",
      "57 train 1.288936e-05 9.117894e-06 3.800821e-05 57 test 1.534828e-05 1.034646e-05 4.817536e-05\n",
      "58 train 1.271847e-05 9.062222e-06 3.707109e-05 58 test 1.568931e-05 1.002210e-05 5.288332e-05\n",
      "59 train 1.296439e-05 9.261699e-06 3.762881e-05 59 test 1.548241e-05 1.046090e-05 4.843869e-05\n",
      "60 train 1.227300e-05 8.714295e-06 3.597597e-05 60 test 1.401156e-05 8.972280e-06 4.708445e-05\n",
      "61 train 1.216026e-05 8.632967e-06 3.565169e-05 61 test 1.396173e-05 8.954787e-06 4.682244e-05\n",
      "62 train 1.215775e-05 8.629549e-06 3.565753e-05 62 test 1.399982e-05 8.955706e-06 4.710450e-05\n",
      "63 train 1.215369e-05 8.622656e-06 3.567229e-05 63 test 1.404439e-05 9.005049e-06 4.711774e-05\n",
      "64 train 1.214059e-05 8.620578e-06 3.558465e-05 64 test 1.403325e-05 9.053280e-06 4.671696e-05\n",
      "65 train 1.212620e-05 8.621881e-06 3.546689e-05 65 test 1.392422e-05 8.941921e-06 4.662318e-05\n",
      "66 train 1.217112e-05 8.652243e-06 3.560307e-05 66 test 1.405135e-05 8.991124e-06 4.726172e-05\n",
      "67 train 1.213800e-05 8.628643e-06 3.551110e-05 67 test 1.394616e-05 8.975303e-06 4.657003e-05\n",
      "68 train 1.213639e-05 8.623948e-06 3.552665e-05 68 test 1.400469e-05 8.968694e-06 4.705604e-05\n",
      "69 train 1.213833e-05 8.633807e-06 3.547925e-05 69 test 1.397701e-05 8.946499e-06 4.699242e-05\n",
      "70 train 1.213098e-05 8.609393e-06 3.558208e-05 70 test 1.405042e-05 8.966822e-06 4.741418e-05\n",
      "71 train 1.211483e-05 8.621465e-06 3.538371e-05 71 test 1.401864e-05 9.009801e-06 4.689179e-05\n",
      "72 train 1.212716e-05 8.632210e-06 3.540886e-05 72 test 1.400419e-05 8.947567e-06 4.719094e-05\n",
      "73 train 1.215831e-05 8.629929e-06 3.565699e-05 73 test 1.417413e-05 9.036626e-06 4.789170e-05\n",
      "74 train 1.212314e-05 8.622316e-06 3.544395e-05 74 test 1.392855e-05 8.949964e-06 4.660317e-05\n",
      "75 train 1.211799e-05 8.615042e-06 3.544501e-05 75 test 1.391541e-05 8.969748e-06 4.637390e-05\n",
      "76 train 1.211483e-05 8.608704e-06 3.546760e-05 76 test 1.402475e-05 9.063936e-06 4.658269e-05\n",
      "77 train 1.209418e-05 8.606790e-06 3.532214e-05 77 test 1.398322e-05 8.974127e-06 4.685805e-05\n",
      "78 train 1.210722e-05 8.611636e-06 3.538746e-05 78 test 1.396955e-05 8.926249e-06 4.706885e-05\n",
      "79 train 1.212925e-05 8.623029e-06 3.548265e-05 79 test 1.407560e-05 9.121253e-06 4.659110e-05\n",
      "80 train 1.210754e-05 8.616119e-06 3.536344e-05 80 test 1.395733e-05 8.911230e-06 4.707501e-05\n",
      "81 train 1.209903e-05 8.615973e-06 3.529587e-05 81 test 1.387903e-05 8.889329e-06 4.662657e-05\n",
      "82 train 1.206523e-05 8.573767e-06 3.531913e-05 82 test 1.404274e-05 8.923102e-06 4.764305e-05\n",
      "83 train 1.206384e-05 8.576893e-06 3.528543e-05 83 test 1.400431e-05 8.978114e-06 4.699133e-05\n",
      "84 train 1.207000e-05 8.582809e-06 3.529318e-05 84 test 1.391431e-05 8.911101e-06 4.675050e-05\n",
      "85 train 1.206715e-05 8.588684e-06 3.523339e-05 85 test 1.394332e-05 8.928642e-06 4.685479e-05\n",
      "86 train 1.205934e-05 8.587055e-06 3.518442e-05 86 test 1.387441e-05 8.895953e-06 4.654812e-05\n",
      "87 train 1.208857e-05 8.595468e-06 3.535572e-05 87 test 1.394086e-05 8.899759e-06 4.702574e-05\n",
      "88 train 1.204625e-05 8.571915e-06 3.518727e-05 88 test 1.388469e-05 8.902993e-06 4.657969e-05\n",
      "89 train 1.208972e-05 8.610400e-06 3.526278e-05 89 test 1.386934e-05 8.879631e-06 4.661696e-05\n",
      "90 train 1.206765e-05 8.570878e-06 3.535351e-05 90 test 1.398800e-05 8.938009e-06 4.713121e-05\n",
      "91 train 1.203712e-05 8.565589e-06 3.515493e-05 91 test 1.399116e-05 9.049886e-06 4.642084e-05\n",
      "92 train 1.207245e-05 8.591607e-06 3.525452e-05 92 test 1.392314e-05 8.900538e-06 4.688658e-05\n",
      "93 train 1.203195e-05 8.562416e-06 3.513987e-05 93 test 1.386629e-05 8.890429e-06 4.652298e-05\n",
      "94 train 1.205916e-05 8.587800e-06 3.517922e-05 94 test 1.387410e-05 8.905892e-06 4.648055e-05\n",
      "95 train 1.201597e-05 8.555135e-06 3.506930e-05 95 test 1.390555e-05 8.906507e-06 4.671443e-05\n",
      "96 train 1.203936e-05 8.570404e-06 3.514117e-05 96 test 1.389027e-05 8.895710e-06 4.666970e-05\n",
      "97 train 1.202321e-05 8.556730e-06 3.511078e-05 97 test 1.386483e-05 8.898143e-06 4.646132e-05\n",
      "98 train 1.202487e-05 8.567992e-06 3.504735e-05 98 test 1.408727e-05 8.907816e-06 4.808015e-05\n",
      "99 train 1.203556e-05 8.568137e-06 3.512596e-05 99 test 1.381283e-05 8.842161e-06 4.643546e-05\n",
      "100 train 1.192935e-05 8.491676e-06 3.482391e-05 100 test 1.382712e-05 8.835466e-06 4.658749e-05\n",
      "101 train 1.192881e-05 8.496745e-06 3.478824e-05 101 test 1.385853e-05 8.836503e-06 4.681821e-05\n",
      "102 train 1.192672e-05 8.490927e-06 3.480986e-05 102 test 1.382014e-05 8.834354e-06 4.654199e-05\n",
      "103 train 1.193027e-05 8.489666e-06 3.484438e-05 103 test 1.381106e-05 8.839093e-06 4.644221e-05\n",
      "104 train 1.192978e-05 8.491842e-06 3.482498e-05 104 test 1.381222e-05 8.827343e-06 4.652810e-05\n",
      "105 train 1.192753e-05 8.493643e-06 3.479914e-05 105 test 1.382450e-05 8.832760e-06 4.658539e-05\n",
      "106 train 1.192465e-05 8.489590e-06 3.480297e-05 106 test 1.380890e-05 8.830228e-06 4.648408e-05\n",
      "107 train 1.192317e-05 8.488268e-06 3.479819e-05 107 test 1.380471e-05 8.828250e-06 4.646536e-05\n",
      "108 train 1.191894e-05 8.485611e-06 3.478459e-05 108 test 1.380605e-05 8.827379e-06 4.648118e-05\n",
      "109 train 1.192431e-05 8.488339e-06 3.480752e-05 109 test 1.381966e-05 8.831570e-06 4.655661e-05\n",
      "110 train 1.192493e-05 8.486161e-06 3.482682e-05 110 test 1.382239e-05 8.830991e-06 4.658107e-05\n",
      "111 train 1.192121e-05 8.489303e-06 3.477846e-05 111 test 1.380536e-05 8.826281e-06 4.648320e-05\n",
      "112 train 1.192222e-05 8.491171e-06 3.477488e-05 112 test 1.382489e-05 8.838069e-06 4.655350e-05\n",
      "113 train 1.192328e-05 8.491466e-06 3.477767e-05 113 test 1.381064e-05 8.826154e-06 4.652394e-05\n",
      "114 train 1.192484e-05 8.489538e-06 3.480137e-05 114 test 1.380113e-05 8.830807e-06 4.642152e-05\n",
      "115 train 1.192027e-05 8.486178e-06 3.479210e-05 115 test 1.380262e-05 8.825306e-06 4.646882e-05\n",
      "116 train 1.192485e-05 8.487765e-06 3.481661e-05 116 test 1.379606e-05 8.825593e-06 4.641733e-05\n",
      "117 train 1.192419e-05 8.487914e-06 3.480947e-05 117 test 1.378988e-05 8.831116e-06 4.633436e-05\n",
      "118 train 1.192328e-05 8.489267e-06 3.479348e-05 118 test 1.384580e-05 8.847417e-06 4.665028e-05\n",
      "119 train 1.192642e-05 8.493179e-06 3.479703e-05 119 test 1.379965e-05 8.823524e-06 4.645813e-05\n",
      "120 train 1.191787e-05 8.485774e-06 3.477756e-05 120 test 1.379413e-05 8.823778e-06 4.641464e-05\n",
      "121 train 1.190992e-05 8.481131e-06 3.474868e-05 121 test 1.387197e-05 8.835361e-06 4.692736e-05\n",
      "122 train 1.192087e-05 8.489494e-06 3.477683e-05 122 test 1.380300e-05 8.823117e-06 4.648607e-05\n",
      "123 train 1.191932e-05 8.482748e-06 3.480655e-05 123 test 1.380824e-05 8.831285e-06 4.647215e-05\n",
      "124 train 1.191494e-05 8.479729e-06 3.479646e-05 124 test 1.379991e-05 8.821123e-06 4.647581e-05\n",
      "125 train 1.191281e-05 8.484718e-06 3.474471e-05 125 test 1.379643e-05 8.825539e-06 4.642052e-05\n",
      "126 train 1.191835e-05 8.487725e-06 3.476599e-05 126 test 1.378891e-05 8.822126e-06 4.638602e-05\n",
      "127 train 1.191443e-05 8.484174e-06 3.476074e-05 127 test 1.381118e-05 8.819637e-06 4.657077e-05\n",
      "128 train 1.191349e-05 8.483681e-06 3.475789e-05 128 test 1.383240e-05 8.828244e-06 4.667477e-05\n",
      "129 train 1.190965e-05 8.481930e-06 3.474350e-05 129 test 1.379206e-05 8.819790e-06 4.642517e-05\n",
      "130 train 1.191324e-05 8.482170e-06 3.476385e-05 130 test 1.380700e-05 8.823970e-06 4.651074e-05\n",
      "131 train 1.191266e-05 8.482483e-06 3.475729e-05 131 test 1.381851e-05 8.819756e-06 4.662545e-05\n",
      "132 train 1.191713e-05 8.485870e-06 3.476788e-05 132 test 1.379695e-05 8.817389e-06 4.647792e-05\n",
      "133 train 1.191300e-05 8.485204e-06 3.474405e-05 133 test 1.379425e-05 8.816899e-06 4.646073e-05\n",
      "134 train 1.191176e-05 8.482659e-06 3.474810e-05 134 test 1.379838e-05 8.820340e-06 4.646935e-05\n",
      "135 train 1.190854e-05 8.480095e-06 3.474497e-05 135 test 1.379460e-05 8.825099e-06 4.640955e-05\n",
      "136 train 1.191516e-05 8.484228e-06 3.476479e-05 136 test 1.380365e-05 8.817690e-06 4.652663e-05\n",
      "137 train 1.190242e-05 8.480127e-06 3.469682e-05 137 test 1.379561e-05 8.834397e-06 4.635619e-05\n",
      "138 train 1.191135e-05 8.482555e-06 3.474457e-05 138 test 1.379902e-05 8.817888e-06 4.649032e-05\n",
      "139 train 1.190109e-05 8.471143e-06 3.474530e-05 139 test 1.378539e-05 8.822751e-06 4.635528e-05\n",
      "140 train 1.190857e-05 8.478849e-06 3.475353e-05 140 test 1.379564e-05 8.814653e-06 4.648595e-05\n",
      "141 train 1.190337e-05 8.476429e-06 3.473204e-05 141 test 1.379835e-05 8.814467e-06 4.650771e-05\n",
      "142 train 1.190783e-05 8.475278e-06 3.476489e-05 142 test 1.380934e-05 8.820417e-06 4.655179e-05\n",
      "143 train 1.190565e-05 8.477677e-06 3.473562e-05 143 test 1.378880e-05 8.816349e-06 4.642312e-05\n",
      "144 train 1.190874e-05 8.476665e-06 3.476717e-05 144 test 1.378163e-05 8.810689e-06 4.640605e-05\n",
      "145 train 1.190299e-05 8.474072e-06 3.473810e-05 145 test 1.379681e-05 8.813233e-06 4.650415e-05\n",
      "146 train 1.191114e-05 8.481535e-06 3.475418e-05 146 test 1.380175e-05 8.811539e-06 4.655261e-05\n",
      "147 train 1.190131e-05 8.473744e-06 3.472853e-05 147 test 1.379097e-05 8.813901e-06 4.645559e-05\n",
      "148 train 1.190445e-05 8.481552e-06 3.469952e-05 148 test 1.380433e-05 8.810814e-06 4.657693e-05\n",
      "149 train 1.190561e-05 8.477867e-06 3.473625e-05 149 test 1.379088e-05 8.810628e-06 4.647638e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_mse,train_mse_0,train_mse_1 = loop(model_ano,dataloader_train,optimizer)\n",
    "    print('%-2d'%epoch,'train','%.6e'%train_mse,'%.6e'%train_mse_0,'%.6e'%train_mse_1,end=' ')\n",
    "    \n",
    "    with torch.no_grad() :\n",
    "        test_mse,test_mse_0,test_mse_1 = loop(model_ano,dataloader_test)\n",
    "        print('%-2d'%epoch,'test','%.6e'%test_mse,'%.6e'%test_mse_0,'%.6e'%test_mse_1)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    ano_loss_list[epoch,0] = train_mse; ano_loss_list[epoch,1] = test_mse\n",
    "    ano_loss_list_0[epoch,0] = train_mse_0; ano_loss_list_0[epoch,1] = test_mse_0\n",
    "    ano_loss_list_1[epoch,0] = train_mse_1; ano_loss_list_1[epoch,1] = test_mse_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
